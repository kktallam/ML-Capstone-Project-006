{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4a7a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import torch\n",
    "from FinABSA import ABSA\n",
    "from tqdm.auto import tqdm # A good progress bar library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21c4734",
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_csv('News/guardian_financial_news_master.csv')\n",
    "news['pub_date'] = pd.to_datetime(news['pub_date'])\n",
    "\n",
    "start_date = '2020-06-30'\n",
    "end_date = '2020-09-30'\n",
    "filtered_df = news.loc[(news['pub_date'] >= start_date) & (news['pub_date'] <= end_date)].copy()\n",
    "filtered_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02f1822",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_model(text, absa):\n",
    "    o = absa.run_absa(input_str = text)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    return o\n",
    "\n",
    "#print(\"\\n--- Running with pandas ---\")\n",
    "absa = ABSA()\n",
    "#filtered_df.loc[:, 'sentiment_results']= filtered_df['body'].apply(sentiment_model, args=(absa,))\n",
    "\n",
    "# 2. Define your batch size\n",
    "# Start small (e.g., 16 or 32). If you don't get memory errors, \n",
    "# you can try increasing it to find the sweet spot.\n",
    "batch_size = 16\n",
    "\n",
    "# 3. Create a list to store all results\n",
    "all_results = []\n",
    "\n",
    "# 4. Loop through the DataFrame in chunks (batches)\n",
    "# This uses tqdm to give you a nice progress bar\n",
    "for i in tqdm(range(0, len(filtered_df), batch_size)):\n",
    "    \n",
    "    # Get a small batch of texts\n",
    "    batch_texts = filtered_df['body'][i : i + batch_size].tolist()\n",
    "\n",
    "    # Process just that batch (you can use a list comprehension)\n",
    "    # Your original 'run_absa' is called here for each item in the *small batch*\n",
    "    batch_results = [absa.run_absa(text) for text in batch_texts]\n",
    "    \n",
    "    # Add the results from this batch to your main list\n",
    "    all_results.extend(batch_results)\n",
    "    \n",
    "    # 5. CRITICAL STEP: Clear the GPU cache\n",
    "    # This frees up memory before the next batch, preventing the OOM error.\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# 6. Add the results back to your DataFrame\n",
    "filtered_df['sentiment_results'] = all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e790b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_series = filtered_df.set_index('pub_date')['sentiment_results'].apply(pd.Series).stack()\n",
    "\n",
    "long_df = long_series.reset_index()\n",
    "long_df.columns = ['date', 'entity', 'details']\n",
    "\n",
    "long_df['classification'] = long_df['details'].apply(lambda x: x['classification_output'])\n",
    "long_df['softmax_scores'] = long_df['details'].apply(lambda x: x['logits'])\n",
    "\n",
    "long_df = long_df.drop(columns='details')\n",
    "\n",
    "print(\"\\n--- Long DataFrame ---\")\n",
    "long_df.to_csv('/home/ktallam/signaltest.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
